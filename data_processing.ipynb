{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff040ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports - DATA PROCESSING\n",
    "# Task 1 - Build Classifier using ResNet-18\n",
    "# Task 2 - Add adversarial Debiasing with GRL\n",
    "# Task 3 - Evaluate Fairness Improvents (accuracy on l, m, d separately), absolute accuracy gaps, Grad-CAM focus changes\n",
    "# Task 4 - Grad-CAM visualizations with basline and debiased model\n",
    "# -- we are looking for basline to focus background lighting or arm position, and debiased to focus more on hand shape only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOUNT THE DRIVE - FOR COLAB\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280098f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"/content/drive/MyDrive/Colab\\Notebooks/data/asldata/root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"asldata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af99137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf data\n",
    "!unzip \"asldata/Root.zip\" -d \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suvethika\n",
    "!ls \"data/Images of American Sign Language (ASL) Alphabet Gestures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4defca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suvethika\n",
    "!unzip \"data/Images of American Sign Language (ASL) Alphabet Gestures/Root.zip\" -d \"data/asl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361fb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# =========================================================\n",
    "# FUNCTION TO MANUALLY LABEL DATA - RUN ONCE\n",
    "# =========================================================\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "TRAIN_ROOT = \"data/train\"     # or your full train path\n",
    "SAMPLES_PER_LETTER = 18\n",
    "CSV_PATH = \"data/asl_manual_labels.csv\"\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. BUILD / LOAD SAMPLE DF\n",
    "# =========================================================\n",
    "def make_sample_df(train_root=TRAIN_ROOT, samples_per_letter=SAMPLES_PER_LETTER, seed=42):\n",
    "    random.seed(seed)\n",
    "    rows = []\n",
    "\n",
    "    for letter in sorted(os.listdir(train_root)):\n",
    "        letter_dir = os.path.join(train_root, letter)\n",
    "        if not os.path.isdir(letter_dir):\n",
    "            continue\n",
    "\n",
    "        imgs = [\n",
    "            os.path.join(letter_dir, f)\n",
    "            for f in os.listdir(letter_dir)\n",
    "            if f.lower().endswith(\".jpg\")\n",
    "        ]\n",
    "        if not imgs:\n",
    "            continue\n",
    "\n",
    "        chosen = random.sample(imgs, min(samples_per_letter, len(imgs)))\n",
    "        for path in chosen:\n",
    "            rows.append({\"filepath\": path, \"letter\": letter, \"skin_tone\": None})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # shuffle so you don't do all A's then all B's\n",
    "    df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# create or load\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"Loaded existing labels from {CSV_PATH}\")\n",
    "else:\n",
    "    df = make_sample_df()\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Created new sample and saved to {CSV_PATH}\")\n",
    "\n",
    "# start at first unlabeled index\n",
    "unlabeled_idx = df[\"skin_tone\"].isna()\n",
    "if unlabeled_idx.any():\n",
    "    index = unlabeled_idx.idxmax()\n",
    "else:\n",
    "    index = 0\n",
    "\n",
    "print(f\"Starting at index {index} of {len(df)} total images.\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. IMAGE DISPLAY AREA\n",
    "# =========================================================\n",
    "image_out = widgets.Output()\n",
    "\n",
    "def show_image(i):\n",
    "    image_out.clear_output(wait=True)\n",
    "    with image_out:\n",
    "        if i >= len(df):\n",
    "            print(\"ðŸŽ‰ All images labeled!\")\n",
    "            return\n",
    "\n",
    "        row = df.iloc[i]\n",
    "        img = Image.open(row.filepath).convert(\"RGB\")\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Index {i} | Letter: {row.letter}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. TEXT INPUT FOR KEYBOARD LABELS\n",
    "# =========================================================\n",
    "label_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='l = light, m = medium, d = dark, s = skip, q = quit',\n",
    "    description='Label:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "status_out = widgets.Output()\n",
    "\n",
    "def handle_label_submission(change):\n",
    "    global index, df\n",
    "    text = change.value.strip().lower()\n",
    "    label_box.value = ''  # clear box for next input\n",
    "\n",
    "    if index >= len(df):\n",
    "        with status_out:\n",
    "            status_out.clear_output(wait=True)\n",
    "            print(\"All images already labeled.\")\n",
    "        return\n",
    "\n",
    "    if text in ['q', 'quit']:\n",
    "        with status_out:\n",
    "            status_out.clear_output(wait=True)\n",
    "            print(f\"Stopped labeling at index {index}. Progress saved to {CSV_PATH}.\")\n",
    "        return\n",
    "\n",
    "    if text in ['l', 'light']:\n",
    "        tone = 'light'\n",
    "    elif text in ['m', 'medium']:\n",
    "        tone = 'medium'\n",
    "    elif text in ['d', 'dark']:\n",
    "        tone = 'dark'\n",
    "    elif text in ['s', 'skip', '']:\n",
    "        tone = None\n",
    "    else:\n",
    "        # invalid input; show help\n",
    "        with status_out:\n",
    "            status_out.clear_output(wait=True)\n",
    "            print(\"Invalid input. Use l/m/d/s/q.\")\n",
    "        return\n",
    "\n",
    "    if tone is not None:\n",
    "        df.at[index, \"skin_tone\"] = tone\n",
    "        df.to_csv(CSV_PATH, index=False)\n",
    "        with status_out:\n",
    "            status_out.clear_output(wait=True)\n",
    "            print(f\"Saved index={index} â†’ {tone}\")\n",
    "    else:\n",
    "        with status_out:\n",
    "            status_out.clear_output(wait=True)\n",
    "            print(f\"Skipped index={index}\")\n",
    "\n",
    "    index += 1\n",
    "    show_image(index)\n",
    "\n",
    "label_box.on_submit(handle_label_submission)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. LAYOUT & START\n",
    "# =========================================================\n",
    "ui = widgets.VBox([\n",
    "    image_out,\n",
    "    label_box,\n",
    "    status_out\n",
    "])\n",
    "\n",
    "display(ui)\n",
    "show_image(index)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CSV exists\n",
    "CSV_PATH = \"data/asl_manual_labels.csv\"\n",
    "print(\"Exists?\", os.path.exists(CSV_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990672c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARK MPS FOR MACBOOK \n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab52719",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_CSV = \"data/asl_manual_labels.csv\"  # path to your labeled subset\n",
    "\n",
    "df_manual = pd.read_csv(MANUAL_CSV)\n",
    "print(df_manual.head())\n",
    "print(df_manual[\"skin_tone\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode skin tone labels\n",
    "skin_tone_to_id = {\"light\": 0, \"medium\": 1, \"dark\": 2}\n",
    "id_to_skin_tone = {v: k for k, v in skin_tone_to_id.items()}\n",
    "\n",
    "df_manual = df_manual[df_manual[\"skin_tone\"].notna()].copy()\n",
    "df_manual[\"skin_tone_id\"] = df_manual[\"skin_tone\"].map(skin_tone_to_id)\n",
    "df_manual[\"skin_tone_id\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55121db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset class for skin tone classification\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class SkinToneDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"filepath\"]).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        label = int(row[\"skin_tone_id\"])\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9227327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloaders\n",
    "full_dataset = SkinToneDataset(df_manual, transform=transform)\n",
    "\n",
    "val_frac = 0.2\n",
    "n_total = len(full_dataset)\n",
    "n_val = int(n_total * val_frac)\n",
    "n_train = n_total - n_val\n",
    "\n",
    "train_ds, val_ds = random_split(full_dataset, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "n_train, n_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18521a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, loss, optimizer\n",
    "num_classes = 3  # light, medium, dark\n",
    "\n",
    "model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "# Replace classifier head\n",
    "in_features = model.classifier[-1].in_features\n",
    "model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_MODEL_PATH = \"skin_tone_mobilenet_best.pth\" # path to save best model for skin tone classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# RUN BEST MODEL TO LABEL 87000 IMAGES\n",
    "# =========================================================\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "num_epochs = 8  # you can tweak this\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = eval_model(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
    "          f\"Train loss: {train_loss:.4f}, acc: {train_acc:.3f} | \"\n",
    "          f\"Val loss: {val_loss:.4f}, acc: {val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  ðŸ”¥ New best model saved with val_acc={val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ROOT = \"data/train\"  # change if your path is different\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for letter in sorted(os.listdir(TRAIN_ROOT)):\n",
    "    letter_dir = os.path.join(TRAIN_ROOT, letter)\n",
    "    if not os.path.isdir(letter_dir):\n",
    "        continue\n",
    "\n",
    "    for fname in os.listdir(letter_dir):\n",
    "        if fname.lower().endswith(\".jpg\"):\n",
    "            fpath = os.path.join(letter_dir, fname)\n",
    "            all_rows.append({\"filepath\": fpath, \"letter\": letter})\n",
    "\n",
    "df_all = pd.DataFrame(all_rows)\n",
    "print(\"Total images:\", len(df_all))\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4873c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllImagesDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"filepath\"]).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, row[\"filepath\"]\n",
    "\n",
    "all_dataset = AllImagesDataset(df_all, transform=transform)\n",
    "all_loader = DataLoader(all_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "pred_skin_ids = []\n",
    "pred_paths = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(all_loader, desc=\"Predicting skin tones\"):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = outputs.max(1)\n",
    "        preds = preds.cpu().numpy().tolist()\n",
    "\n",
    "        pred_skin_ids.extend(preds)\n",
    "        pred_paths.extend(list(paths))\n",
    "\n",
    "len(pred_skin_ids), len(pred_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e04988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame({\n",
    "    \"filepath\": pred_paths,\n",
    "    \"skin_tone_id_pred\": pred_skin_ids\n",
    "})\n",
    "df_pred[\"skin_tone_pred\"] = df_pred[\"skin_tone_id_pred\"].map(id_to_skin_tone)\n",
    "\n",
    "df_pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 3. merge all the data\n",
    "# =========================================================\n",
    "# 1) Make sure we have fresh copies\n",
    "df_all = df_all.copy()       # ['filepath', 'letter']\n",
    "df_pred = df_pred.copy()     # ['filepath', 'skin_tone_id_pred', 'skin_tone_pred']\n",
    "df_manual = df_manual.copy() # ['filepath', 'letter', 'skin_tone', 'skin_tone_id']\n",
    "\n",
    "# 2) Keep only the manual label columns we need (avoid letter conflict)\n",
    "df_manual_small = df_manual[[\"filepath\", \"skin_tone\", \"skin_tone_id\"]].copy()\n",
    "\n",
    "# 3) Rename manual columns so we can distinguish them\n",
    "df_manual_small = df_manual_small.rename(columns={\n",
    "    \"skin_tone\": \"skin_tone_manual\",\n",
    "    \"skin_tone_id\": \"skin_tone_id_manual\"\n",
    "})\n",
    "\n",
    "# 4) First merge: all images + predicted labels\n",
    "df_merged = df_all.merge(df_pred, on=\"filepath\", how=\"left\")\n",
    "\n",
    "# 5) Second merge: add manual labels where available\n",
    "df_merged = df_merged.merge(df_manual_small, on=\"filepath\", how=\"left\")\n",
    "\n",
    "# 6) Create final label columns:\n",
    "#    - if a manual label exists, use that\n",
    "#    - otherwise use the predicted label\n",
    "df_merged[\"skin_tone_final\"] = df_merged[\"skin_tone_manual\"].combine_first(\n",
    "    df_merged[\"skin_tone_pred\"]\n",
    ")\n",
    "\n",
    "df_merged[\"skin_tone_id_final\"] = df_merged[\"skin_tone_id_manual\"].combine_first(\n",
    "    df_merged[\"skin_tone_id_pred\"]\n",
    ")\n",
    "\n",
    "# 7) Quick sanity checks\n",
    "print(df_merged[[\"filepath\", \"letter\", \"skin_tone_pred\", \"skin_tone_manual\", \"skin_tone_final\"]].head())\n",
    "print(\"\\nCounts of final labels:\")\n",
    "print(df_merged[\"skin_tone_final\"].value_counts(dropna=False))\n",
    "\n",
    "# 8) Save out final CSV\n",
    "OUT_CSV = \"data/asl_all_with_skin_tones.csv\"\n",
    "df_merged.to_csv(OUT_CSV, index=False)\n",
    "print(\"\\nSaved:\", OUT_CSV, \"with\", len(df_merged), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER USING RESNET-18\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if CSV_PATH = \"data/asl_all_with_skin_tones.csv\" exists\n",
    "CSV_PATH = \"data/asl_all_with_skin_tones.csv\"\n",
    "print(\"Exists?\", os.path.exists(CSV_PATH))\n",
    "\n",
    "#why is it empty?\n",
    "df_all_skin = pd.read_csv(CSV_PATH)\n",
    "print(df_all_skin.head())\n",
    "print(df_all_skin[\"skin_tone_final\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c608fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"data/asl_all_with_skin_tones.csv\"  # adjust if needed\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# We only need these columns for baseline\n",
    "df = df[[\"filepath\", \"letter\"]].dropna()\n",
    "\n",
    "# Make sure paths exist (optional but nice sanity check)\n",
    "df = df[df[\"filepath\"].apply(os.path.exists)]\n",
    "print(\"Total images after path check:\", len(df))\n",
    "\n",
    "# Map letters to integer IDs\n",
    "letters = sorted(df[\"letter\"].unique())\n",
    "letter_to_id = {letter: idx for idx, letter in enumerate(letters)}\n",
    "id_to_letter = {v: k for k, v in letter_to_id.items()}\n",
    "\n",
    "df[\"label_id\"] = df[\"letter\"].map(letter_to_id)\n",
    "\n",
    "print(\"Num classes:\", len(letters))\n",
    "print(\"Classes:\", letters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99829c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a539ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34dc2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
